"""
Module principal pour les pr√©dictions et trading d'actions
"""
import os
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import sys
import pickle
import numpy as np
import pandas as pd
import gc
import json
import logging
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
warnings.filterwarnings('ignore')

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model

from sklearn.preprocessing import StandardScaler

# Modules internes
from trading_algo.data.data_extraction import StockDataExtractor, get_stock_overview, MacroDataExtractor
from trading_algo.models.base_model import ImprovedLSTMPredictorMultiOutput

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
tf.get_logger().setLevel('ERROR')


class StockModelTrain:
    """
    Classe principale pour les pr√©dictions d'actions avec IA
    Int√®gre les modules data_extraction et dashboard
    """
    
    def __init__(self, symbol: str, period: str = "3y"):
        self.symbol = symbol
        self.period = period
        self.model = None
        self.data = None
        self.features = None
        self.feature_columns = None
        self.targets = None
        self.target_columns = None
        self.current_price = 0.0
        
        # Extracteurs de donn√©es
        self.data_extractor = StockDataExtractor(symbol)
        self.macro_extractor = MacroDataExtractor()
        
        # Scalers
        self.feature_scaler = StandardScaler()
        self.target_scaler = StandardScaler()
        
        # Pr√©dictions
        self.predictions_1d = None
        self.predictions_30d = None
        self.predictions_90d = None
        
        # R√©sultats d'analyse
        self.analysis_results = {}
        self.trading_score = 5.0
        self.recommendation = "NEUTRE"
        
        # Configuration
        self.sequence_length = 60  # valeur par d√©faut, sera mise √† jour pendant l'entra√Ænement
        self.lookback_days = 60     # alias pour coh√©rence
        self.batch_size = 32
        self.patience_early_stopping = 20
        
        logger.info(f"StockModelTrain initialis√© pour {symbol}")
    
    def _check_gpu(self):
        """V√©rifie et configure les GPUs disponibles"""
        try:
            gpus = tf.config.list_physical_devices('GPU')
            if gpus:
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                logger.info(f"GPU d√©tect√©: {len(gpus)} disponible(s)")
                return True
            else:
                logger.info("Aucun GPU d√©tect√©, utilisation du CPU")
                return False
        except Exception as e:
            logger.warning(f"Erreur configuration GPU: {e}")
            return False
    
    def fetch_data(self, forecast_horizon: int = 1) -> bool:
        """
        R√©cup√®re et pr√©pare les donn√©es pour l'analyse
        """
        try:
            logger.info(f"R√©cup√©ration des donn√©es pour {self.symbol}")
            
            all_data = self.data_extractor.get_all_data(
                symbol=self.symbol,
                period=self.period
            )
            
            if all_data is None or not isinstance(all_data, dict) or 'historical' not in all_data:
                logger.error("Donn√©es historiques manquantes")
                return False
            
            historical_data = all_data['historical']
            if historical_data is None or historical_data.empty:
                logger.error("Donn√©es historiques vides")
                return False
            
            self.data = historical_data
            
            if 'technical' in all_data and all_data['technical'] is not None and not all_data['technical'].empty:
                self.features = all_data['technical']
            else:
                self.features = self.data_extractor.calculate_technical_indicators(self.data)
            
            if 'targets' in all_data and all_data['targets'] is not None and not all_data['targets'].empty:
                self.targets = all_data['targets']
            else:
                self.targets = self.data_extractor.create_target_columns(
                    self.features, 
                    forecast_days=[1, 5, 10, 20, 30, 90]
                )
            
            if self.targets is None or self.targets.empty:
                logger.error("√âchec de la cr√©ation des cibles")
                return False
            
            if not self.data.empty and 'Close' in self.data.columns:
                self.current_price = float(self.data['Close'].iloc[-1])
            
            if self.features is not None and not self.features.empty:
                self.feature_scaler.fit(self.features)
            
            if self.targets is not None and not self.targets.empty:
                self.target_scaler.fit(self.targets.dropna())
            
            logger.info(f"Donn√©es r√©cup√©r√©es: {len(self.data)} p√©riodes")
            logger.info(f"Features: {self.features.shape}")
            logger.info(f"Cibles: {self.targets.shape}")
            logger.info(f"Prix actuel: ${self.current_price:.2f}")
            
            return True
            
        except Exception as e:
            logger.error(f"Erreur fetch_data: {e}", exc_info=True)
            return False
    
    def _check_data_alignment(self):
        """V√©rifie l'alignement entre data et features"""
        try:
            if self.data is None or self.features is None:
                logger.warning("Donn√©es ou features manquantes")
                return False
            
            data_dates = set(self.data.index)
            feature_dates = set(self.features.index)
            common_dates = data_dates.intersection(feature_dates)
            
            logger.info(f"Dates data: {len(data_dates)}, Dates features: {len(feature_dates)}, "
                       f"Dates communes: {len(common_dates)}")
            
            if len(common_dates) < min(len(data_dates), len(feature_dates)) * 0.8:
                logger.warning(f"Alignement faible: {len(common_dates)/len(data_dates)*100:.1f}%")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"Erreur _check_data_alignment: {e}")
            return False
    
    def prepare_training_data(self, lookback_days: int = 30, train_split: float = 0.8) -> Tuple:
        """
        Pr√©pare les donn√©es pour l'entra√Ænement du mod√®le
        """
        try:
            if self.features is None or self.targets is None:
                logger.error("Donn√©es non disponibles")
                return None, None, None, None
            
            self._check_data_alignment()
            
            X_df = self.features.copy()
            y_df = self.targets.copy()
            
            common_index = X_df.index.intersection(y_df.index)
            X_df = X_df.loc[common_index]
            y_df = y_df.loc[common_index]
            
            X_df.replace([np.inf, -np.inf], np.nan, inplace=True)
            y_df.replace([np.inf, -np.inf], np.nan, inplace=True)
            
            X_df = X_df.ffill().bfill()
            y_df = y_df.ffill().bfill()
            
            split_idx = int(len(X_df) * train_split)
            
            X_train_df = X_df.iloc[:split_idx]
            X_test_df = X_df.iloc[split_idx:]
            y_train_df = y_df.iloc[:split_idx]
            y_test_df = y_df.iloc[split_idx:]
            
            # Apr√®s avoir pr√©par√© X_train_df (avant scaling), vers la fin de la m√©thode train()
            self.feature_columns = X_train_df.columns.tolist()
            logger.info(f"Colonnes des features sauvegard√©es: {len(self.feature_columns)}")
            # Apr√®s avoir pr√©par√© y_train_df, avant le scaling
            self.target_columns = y_train_df.columns.tolist()
            logger.info(f"Colonnes cibles sauvegard√©es: {len(self.target_columns)}")
            if len(X_train_df) < lookback_days + 10:
                logger.warning("Donn√©es insuffisantes pour l'entra√Ænement")
                return None, None, None, None
            
            X_train = self.feature_scaler.transform(X_train_df.values)
            X_test = self.feature_scaler.transform(X_test_df.values)
            
            y_train = self.target_scaler.transform(y_train_df.values)
            y_test = self.target_scaler.transform(y_test_df.values)
            
            X_train_seq, y_train_seq = self._create_sequences(X_train, y_train, lookback_days)
            X_test_seq, y_test_seq = self._create_sequences(X_test, y_test, lookback_days)
            
            logger.info(f"S√©quences d'entra√Ænement: {X_train_seq.shape}")
            logger.info(f"S√©quences de test: {X_test_seq.shape}")
            
            return X_train_seq, y_train_seq, X_test_seq, y_test_seq
            
        except Exception as e:
            logger.error(f"Erreur prepare_training_data: {e}")
            return None, None, None, None
    
    def _create_sequences(self, X: np.ndarray, y: np.ndarray, lookback_days: int) -> Tuple:
        """Cr√©e des s√©quences pour les mod√®les LSTM"""
        if len(X) <= lookback_days:
            return np.array([]).reshape(0, lookback_days, X.shape[1]), np.array([]).reshape(0, y.shape[1])
        
        X_seq = np.zeros((len(X) - lookback_days, lookback_days, X.shape[1]))
        y_seq = np.zeros((len(X) - lookback_days, y.shape[1]))
        
        for i in range(lookback_days, len(X)):
            X_seq[i - lookback_days] = X[i - lookback_days:i]
            y_seq[i - lookback_days] = y[i]
        
        return X_seq, y_seq
    
    def train(self, lookback_days: int = 30, epochs: int = 50, batch_size: int = 32) -> bool:
        """
        Entra√Æne le mod√®le de pr√©diction avec am√©liorations
        """
        try:
            # Gestion m√©moire
            tf.keras.backend.clear_session()
            gc.collect()
            
            # Sauvegarde du lookback_days pour l'utiliser en pr√©diction
            self.lookback_days = lookback_days
            self.sequence_length = lookback_days  # pour compatibilit√©
            
            # Configuration des dossiers
            model_dir = f"models_saved/{self.symbol}"
            checkpoints_dir = f"checkpoints/{self.symbol}"
            log_dir = f"logs/{self.symbol}"
            
            os.makedirs(model_dir, exist_ok=True)
            os.makedirs(checkpoints_dir, exist_ok=True)
            os.makedirs(log_dir, exist_ok=True)
            
            self._check_gpu()
            
            X_train, y_train, X_test, y_test = self.prepare_training_data(lookback_days)
            
            if X_train is None or X_train.shape[0] == 0:
                logger.error("Donn√©es insuffisantes pour l'entra√Ænement")
                return False
            
            if X_train.shape[0] < batch_size * 2:
                new_batch_size = max(1, X_train.shape[0] // 4)
                logger.warning(f"Batch size ajust√© de {batch_size} √† {new_batch_size}")
                batch_size = new_batch_size
            
            n_outputs = y_train.shape[1]
            logger.info(f"Donn√©es d'entra√Ænement: {X_train.shape[0]} √©chantillons, {n_outputs} cibles")
            logger.info(f"Donn√©es de test: {X_test.shape[0]} √©chantillons")
            
            input_shape = (X_train.shape[1], X_train.shape[2])
            
            if X_train.shape[0] < 1000:
                lstm_units1, lstm_units2 = 32, 16
                logger.info("Architecture l√©g√®re s√©lectionn√©e (petit dataset)")
            else:
                lstm_units1, lstm_units2 = 64, 32
            
            self.model = ImprovedLSTMPredictorMultiOutput(
                lstm_units1=lstm_units1,
                lstm_units2=lstm_units2,
                dense_units=32,
                dropout_rate=0.5,
                recurrent_dropout=0.2,
                n_outputs=n_outputs
            )
            
            optimizer = tf.keras.optimizers.Adam(
                learning_rate=0.001,
                beta_1=0.9,
                beta_2=0.999,
                epsilon=1e-07,
                clipnorm=1.0
            )
            
            self.model.compile(
                optimizer=optimizer,
                loss="huber",
                metrics=["mae", "mse"]
            )
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            callbacks = [
                tf.keras.callbacks.ModelCheckpoint(
                    filepath=f"{checkpoints_dir}/best_model_{timestamp}.keras",
                    monitor='val_loss',
                    save_best_only=True,
                    mode='min',
                    verbose=1
                ),
                tf.keras.callbacks.EarlyStopping(
                    monitor='val_loss',
                    patience=self.patience_early_stopping,
                    min_delta=0.0001,
                    restore_best_weights=True,
                    verbose=1
                ),
                tf.keras.callbacks.ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.2,
                    patience=8,
                    min_lr=1e-7,
                    verbose=1
                ),
                tf.keras.callbacks.TensorBoard(
                    log_dir=f"{log_dir}/{timestamp}",
                    histogram_freq=1
                )
            ]
            
            logger.info(f"D√©but de l'entra√Ænement pour {epochs} epochs")
            start_time = datetime.now()
            
            history = self.model.fit(
                X_train, y_train,
                validation_data=(X_test, y_test),
                epochs=epochs,
                batch_size=batch_size,
                callbacks=callbacks,
                shuffle=False,
                verbose=1
            )
            
            training_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"Temps d'entra√Ænement: {training_time:.2f} secondes")
            
            train_metrics = self.model.evaluate(X_train, y_train, verbose=0, return_dict=True)
            val_metrics = self.model.evaluate(X_test, y_test, verbose=0, return_dict=True)
            
            model_path = f"{model_dir}/{self.symbol}_model_{timestamp}.keras"
            self.model.save(model_path)
            logger.info(f"Mod√®le sauvegard√©: {model_path}")
            
            feature_scaler_path = f"{model_dir}/{self.symbol}_feature_scaler_{timestamp}.pkl"
            with open(feature_scaler_path, 'wb') as f:
                pickle.dump(self.feature_scaler, f)
            
            target_scaler_path = f"{model_dir}/{self.symbol}_target_scaler_{timestamp}.pkl"
            with open(target_scaler_path, 'wb') as f:
                pickle.dump(self.target_scaler, f)
            
            metrics = {
                'symbol': self.symbol,
                'timestamp': timestamp,
                'training_time_seconds': training_time,
                'train_samples': len(X_train),
                'test_samples': len(X_test),
                'train_loss': float(train_metrics['loss']),
                'train_mae': float(train_metrics['mae']),
                'val_loss': float(val_metrics['loss']),
                'val_mae': float(val_metrics['mae']),
                'input_shape': input_shape,
                'n_outputs': n_outputs,
                'lookback_days': lookback_days,
                'batch_size': batch_size,
                'architecture': f"LSTM_{lstm_units1}_{lstm_units2}",
                'model_version': '1.0.0'
            }
            
            self._save_metrics(metrics, model_dir)
            self._plot_training_history(history, model_dir, timestamp)

            del X_train, y_train, X_test, y_test
            gc.collect()
            
            logger.info("Entra√Ænement termin√© avec succ√®s")
            return True
            
        except Exception as e:
            logger.error(f"Erreur lors de l'entra√Ænement: {e}", exc_info=True)
            return False
    
    def _save_metrics(self, metrics: Dict[str, Any], save_dir: str):
        """Sauvegarde les m√©triques d'entra√Ænement"""
        try:
            os.makedirs(save_dir, exist_ok=True)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"{save_dir}/metrics_{timestamp}.json"
            with open(filename, 'w') as f:
                json.dump(metrics, f, indent=2)
            logger.info(f"M√©triques sauvegard√©es: {filename}")
        except Exception as e:
            logger.error(f"Erreur save_metrics: {e}")
    
    def _plot_training_history(self, history, model_dir, timestamp):
        """G√©n√®re et sauvegarde des graphiques d'entra√Ænement"""
        try:
            import matplotlib.pyplot as plt
            fig, axes = plt.subplots(1, 2, figsize=(12, 4))
            axes[0].plot(history.history['loss'], label='Train Loss')
            axes[0].plot(history.history['val_loss'], label='Validation Loss')
            axes[0].set_title('Model Loss')
            axes[0].set_xlabel('Epoch')
            axes[0].set_ylabel('Loss')
            axes[0].legend()
            axes[0].grid(True)
            
            axes[1].plot(history.history['mae'], label='Train MAE')
            axes[1].plot(history.history['val_mae'], label='Validation MAE')
            axes[1].set_title('Model MAE')
            axes[1].set_xlabel('Epoch')
            axes[1].set_ylabel('MAE')
            axes[1].legend()
            axes[1].grid(True)
            
            plt.tight_layout()
            plot_path = f"{model_dir}/training_history_{timestamp}.png"
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            plt.close()
            logger.info(f"Graphique d'entra√Ænement sauvegard√©: {plot_path}")
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration du graphique: {e}")
    
    def analyze_model_stock(self) -> Dict[str, Any]:
        """
        Analyse compl√®te de l'action
        """
        try:
            logger.info(f"Analyse de {self.symbol}")
            
            if not self.fetch_data():
                return {"error": "√âchec de la r√©cup√©ration des donn√©es", "symbol": self.symbol}
            
            logger.info("Entra√Ænement du mod√®le IA...")
            training_success = self.train(epochs=30, lookback_days=60)
            
            if not training_success:
                return {
                    "error": "√âchec de l'entra√Ænement du mod√®le",
                    "symbol": self.symbol,
                    "current_price": self.current_price
                }
            
            logger.info("G√©n√©ration des pr√©dictions...")
            predictions = self.generate_predictions()
            
            self.trading_score = self.calculate_trading_score(predictions)
            self.recommendation = self.generate_recommendation(self.trading_score)
            
            self.analysis_results = {
                "symbol": self.symbol,
                "current_price": self.current_price,
                "trading_score": self.trading_score,
                "recommendation": self.recommendation,
                "predictions": predictions,
                "analysis_date": datetime.now().isoformat(),
                "period": self.period
            }
            
            logger.info(f"Analyse termin√©e pour {self.symbol}")
            logger.info(f"Score: {self.trading_score}/10 - {self.recommendation}")
            
            return self.analysis_results
            
        except Exception as e:
            logger.error(f"Erreur analyze_model_stock: {e}", exc_info=True)
            return {"error": str(e), "symbol": self.symbol}
    
    def generate_predictions(self) -> Dict[str, Any]:
        try:
            if self.model is None or self.features is None:
                logger.error("Mod√®le ou features non disponibles")
                return {}
        
            if not hasattr(self, 'feature_columns'):
                logger.error("Aucune colonne de features sauvegard√©e. R√©entra√Ænez le mod√®le.")
                return {}
        
            missing = set(self.feature_columns) - set(self.features.columns)
            if missing:
                logger.error(f"Colonnes manquantes dans features: {missing}")
                return {}
        
            features_aligned = self.features[self.feature_columns]
        
            if len(features_aligned) < self.lookback_days:
                logger.error(f"Pas assez de donn√©es : besoin de {self.lookback_days} jours, disponible {len(features_aligned)}")
                return {}
        
            recent_data = features_aligned.iloc[-self.lookback_days:]
            scaled_data = self.feature_scaler.transform(recent_data)
            X_pred = scaled_data.reshape(1, self.lookback_days, -1)
        
            y_pred_scaled = self.model.predict(X_pred, verbose=0)
        
            if not hasattr(self, 'target_columns'):
                logger.error("target_columns non d√©finies. Utilisation de l'ordre par d√©faut (risqu√©).")
                y_pred = self.target_scaler.inverse_transform(y_pred_scaled)
                return {
                    "1d": float(y_pred[0, 0]) if y_pred.shape[1] > 0 else None,
                    "5d": float(y_pred[0, 1]) if y_pred.shape[1] > 1 else None,
                    "10d": float(y_pred[0, 2]) if y_pred.shape[1] > 2 else None,
                    "20d": float(y_pred[0, 3]) if y_pred.shape[1] > 3 else None,
                    "30d": float(y_pred[0, 4]) if y_pred.shape[1] > 4 else None,
                    "90d": float(y_pred[0, 5]) if y_pred.shape[1] > 5 else None
                }
        
            y_pred = self.target_scaler.inverse_transform(y_pred_scaled)
            pred_dict = {col: y_pred[0, i] for i, col in enumerate(self.target_columns)}
        
            horizons = [1, 5, 10, 20, 30, 90]
            predictions = {}
            for days in horizons:
                close_col = f'Target_Close_{days}d'
                predictions[f'{days}d'] = float(pred_dict[close_col]) if close_col in pred_dict else None
        
            logger.info(f"Pr√©dictions extraites: {predictions}")
            return predictions
        
        except Exception as e:
            logger.error(f"Erreur generate_predictions: {e}")
            return {}
    
    def calculate_trading_score(self, predictions: Dict[str, Any]) -> float:
        """
        Calcule un score de trading bas√© sur les pr√©dictions
        """
        try:
            score = 5.0
            if not predictions:
                return score
            
            returns = {}
            for horizon, pred in predictions.items():
                if pred is not None and self.current_price > 0:
                    returns[horizon] = (pred - self.current_price) / self.current_price * 100
            
            weights = {"1d": 0.1, "5d": 0.15, "10d": 0.2, "30d": 0.3, "90d": 0.25}
            weighted_return = 0
            total_weight = 0
            
            for horizon, weight in weights.items():
                if horizon in returns and returns[horizon] is not None:
                    weighted_return += returns[horizon] * weight
                    total_weight += weight
            
            if total_weight > 0:
                avg_return = weighted_return / total_weight
                score = 5 + (avg_return / 2)
                score = max(0, min(10, score))
            
            return round(score, 2)
            
        except Exception as e:
            logger.error(f"Erreur calculate_trading_score: {e}")
            return 5.0
    
    def generate_recommendation(self, score: float) -> str:
        """
        G√©n√®re une recommandation bas√©e sur le score
        """
        if score >= 7.5:
            return "ACHAT FORT"
        elif score >= 6:
            return "ACHAT"
        elif score >= 4:
            return "NEUTRE"
        elif score >= 2.5:
            return "VENTE"
        else:
            return "VENTE FORTE"
    
    def create_dashboard(self):
        """
        Cr√©e un tableau de bord interactif (√† impl√©menter avec le module dashboard)
        """
        try:
            if not self.analysis_results:
                self.analyze_model_stock()
            
            print(f"\nüìä DASHBOARD POUR {self.symbol}")
            print("=" * 60)
            print(f"Prix actuel: ${self.current_price:.2f}")
            print(f"Score de trading: {self.trading_score}/10")
            print(f"Recommandation: {self.recommendation}")
            print("\nPr√©dictions:")
            
            predictions = self.analysis_results.get('predictions', {})
            for horizon, pred in predictions.items():
                if pred is not None:
                    change_pct = ((pred - self.current_price) / self.current_price * 100)
                    print(f"  {horizon}: ${pred:.2f} ({change_pct:+.2f}%)")
            
            print("\nNote: Le dashboard interactif sera disponible avec le module dashboard.py")
            
        except Exception as e:
            logger.error(f"Erreur stockmodeltrain.create_dashboard: {e}")


def main():
    """Fonction principale d'ex√©cution dans stockmodeltrain"""
    print("üöÄ PR√âDICTEUR D'ACTIONS AVEC IA")
    print("="*60)
    
    popular_stocks = {
        '1': 'AAPL', '2': 'TSLA', '3': 'MSFT', '4': 'GOOGL',
        '5': 'AMZN', '6': 'META', '7': 'NVDA', '8': 'BTC-USD', '9': 'SPY',
    }
    
    print("\nüìà ACTIONS POPULAIRES:")
    for key, value in popular_stocks.items():
        print(f" {key}. {value}")
    print(" 0. Entrer un symbole personnalis√©")
    
    choice = input("\nChoisissez une action (1-9) ou 0 pour personnalis√©: ").strip()
    
    if choice == '0':
        symbol = input("Entrez le symbole de l'action (ex: AAPL): ").strip().upper()
    elif choice in popular_stocks:
        symbol = popular_stocks[choice]
    else:
        print("Choix invalide, utilisation de AAPL par d√©faut")
        symbol = 'AAPL'
    
    print("\n‚è∞ P√âRIODE D'ANALYSE:")
    print(" 1. 1 an")
    print(" 2. 3 ans")
    print(" 3. 5 ans (recommand√©)")
    print(" 4. 10 ans")
    
    period_choice = input("Choisissez la p√©riode (1-4): ").strip()
    periods = {'1': '1y', '2': '3y', '3': '5y', '4': '10y'}
    period = periods.get(period_choice, '5y')
    
    print("\n‚öôÔ∏è OPTIONS D'ANALYSE:")
    print(" 1. Analyse compl√®te avec dashboard")
    print(" 2. Analyse rapide (sans dashboard)")
    
    option_choice = input("Choisissez l'option (1-2): ").strip()
    
    print(f"\n{'='*60}")
    print(f"Lancement de l'analyse pour {symbol} ({period})...")
    print(f"{'='*60}")
    
    start_time = datetime.now()
    
    try:
        predictor = StockModelTrain(symbol, period)
        results = predictor.analyze_model_stock()
        
        if 'error' not in results:
            print(f"\nüìä R√âSULTATS POUR {symbol}:")
            print(f"   Prix actuel: ${results['current_price']:.2f}")
            print(f"   Score de trading: {results['trading_score']:.1f}/10")
            print(f"   Recommandation: {results['recommendation']}")
            
            predictions = results['predictions']
            print(f"\n   Pr√©dictions de prix:")
            for horizon, pred in predictions.items():
                if pred is not None:
                    change_pct = ((pred - results['current_price']) / results['current_price'] * 100)
                    print(f"     {horizon}: ${pred:.2f} ({change_pct:+.2f}%)")
            
            if option_choice == '1':
                print("\nüìà Cr√©ation du dashboard...")
                predictor.create_dashboard()
        else:
            print(f"‚ùå Erreur: {results['error']}")
    
    except Exception as e:
        print(f"‚ùå Erreur lors de l'analyse: {e}")
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    print(f"\n‚è±Ô∏è Analyse termin√©e en {duration:.1f} secondes")
    print("="*60)


if __name__ == "__main__":
    main()